{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install\n"
      ],
      "metadata": {
        "id": "pplw4tTYulPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pR2BNZRsA1r",
        "outputId": "5dfb744a-e4da-4112-ffa9-2777d82d792b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade \"pip\" #if the version of python is not 3.10.12 need to run this code"
      ],
      "metadata": {
        "id": "2uLloazKwPZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf3SvffEshf0",
        "outputId": "3a8666d9-cdbf-4cba-e6c2-b4965d419cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/59.7 kB\u001b[0m \u001b[31m765.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m882.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.5.0)\n",
            "Installing collected packages: h11, uvicorn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 uvicorn-0.24.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iETsnEYKtZeD",
        "outputId": "f359de86-9b35-47dc-c7d4-da33d1616d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m633.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-multipart\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed python-multipart-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Xo9Fb3tg0T",
        "outputId": "8f9f253d-bc79-4670-a83e-cdbf9104eb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kaleido-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH8QdjGWthzw",
        "outputId": "fb1747fc-7c7d-4506-ea8a-e30abc9391b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m998.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
            "Installing collected packages: typing-extensions, starlette, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.104.1 starlette-0.27.0 typing-extensions-4.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install jax==0.4.19\n",
        "!pip install jax==0.4.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNhQcGaNtiCD",
        "outputId": "7843899e-b769-4d45-8a97-d8aeeaac9a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.4.14\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14) (1.11.4)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535362 sha256=1c0d2c078ad94095d32078e8bfe1b40492080dc64a78fb42e525714e536c54a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "Successfully built jax\n",
            "Installing collected packages: jax\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.20\n",
            "    Uninstalling jax-0.4.20:\n",
            "      Successfully uninstalled jax-0.4.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.7.5 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portpicker==1.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "9SzX5zSHtiFT",
        "outputId": "38ba69b0-22f0-4485-f1ff-b8c73d661c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portpicker==1.6.0\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker==1.6.0) (5.9.5)\n",
            "Installing collected packages: portpicker\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed portpicker-1.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "portpicker"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typing-extensions==4.5.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bfERV7uOXl",
        "outputId": "060eeeb8-55f1-43c1-b72e-b98c7ebaf40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions==4.5.*\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.8.0\n",
            "    Uninstalling typing_extensions-4.8.0:\n",
            "      Successfully uninstalled typing_extensions-4.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastapi 0.104.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "flax 0.7.5 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sqlalchemy==1.4\n",
        "!pip install sqlalchemy==2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94kCThdYtiJz",
        "outputId": "9890754c-10c9-48e6-e0fe-ca0eb7f55986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sqlalchemy==2.0\n",
            "  Downloading SQLAlchemy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==2.0) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==2.0) (3.0.1)\n",
            "Installing collected packages: sqlalchemy\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.23\n",
            "    Uninstalling SQLAlchemy-2.0.23:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.23\n",
            "Successfully installed sqlalchemy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDagaGvsNW3k",
        "outputId": "30979b5a-c451-4a51-8dee-9ffe8da4ba8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for farmhashpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.7.5 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "np.random.seed(0)\n",
        "tff.federated_computation(lambda: 'Hello World')()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ABliClvJNd",
        "outputId": "878080a4-beb5-4ab0-81d2-d14bd6df27f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello World'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "J4Nws8G12Lgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# connect drive"
      ],
      "metadata": {
        "id": "hMMw3w_7v-Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')#連結到雲端"
      ],
      "metadata": {
        "id": "QtZr1Q5Zv-LH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08801f45-5cb5-482f-976c-881426ae399f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model test"
      ],
      "metadata": {
        "id": "TEYm2hm4v7-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\"gdrive/My Drive/AI_S_HW4\",batch_size=1, image_size=(28,28))\n",
        "\n",
        "#split dataset into three datasets\n",
        "client1_size = int(len(dataset)*.3)\n",
        "client2_size = int(len(dataset)*.3)\n",
        "client3_size = int(len(dataset)*.3)\n",
        "client4_size = int(len(dataset)*.1)\n",
        "client1_dataset = dataset.take(client1_size)\n",
        "client2_dataset = dataset.skip(client1_size).take(client2_size)\n",
        "client3_dataset = dataset.skip(client1_size+client2_size).take(client3_size)\n",
        "client4_dataset = dataset.skip(client1_size+client2_size+client3_size).take(client4_size)\n",
        "####\n",
        "NUM_CLIENTS = 3\n",
        "####\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element1,element2):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return (tf.reshape(element1, [28, 28, 3]),\n",
        "            tf.reshape(element2, [1]))\n",
        "\n",
        "  return dataset.map(batch_format_fn).batch(20).repeat(10)\n",
        "\n",
        "####\n",
        "preprocessed_client1_dataset = preprocess(client1_dataset)\n",
        "preprocessed_client2_dataset = preprocess(client2_dataset)\n",
        "preprocessed_client3_dataset = preprocess(client3_dataset)\n",
        "preprocessed_client4_dataset = preprocess(client4_dataset)\n",
        "\n",
        "federated_train_data = [\n",
        "      preprocessed_client1_dataset,\n",
        "      preprocessed_client2_dataset,\n",
        "      preprocessed_client3_dataset\n",
        "  ]\n",
        "print(federated_train_data)\n",
        "print(f'Number of client datasets: {len(federated_train_data)}')\n",
        "print(f'First dataset: {federated_train_data[0]}')\n",
        "print(federated_train_data[0].element_spec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RsKS3mjy59e",
        "outputId": "6655c35d-8067-4c4c-fbeb-d2b02e0b5920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5544 files belonging to 2 classes.\n",
            "[<_RepeatDataset element_spec=(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>, <_RepeatDataset element_spec=(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>, <_RepeatDataset element_spec=(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>]\n",
            "Number of client datasets: 3\n",
            "First dataset: <_RepeatDataset element_spec=(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>\n",
            "(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model():\n",
        "  model = Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=(28,28,3))),\n",
        "  model.add(Conv2D(32, kernel_size=(5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(4, activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "3QHXohMDy791"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "  print(keras_model.summary())\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=federated_train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "####"
      ],
      "metadata": {
        "id": "OFY5UO-WzC3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=1e-2))\n",
        "'''\n",
        "####\n",
        "print(training_process.initialize.type_signature.formatted_representation())\n",
        "state = training_process.initialize()\n",
        "for round_num in range(30):\n",
        "  #state, metrics = training_process.next(state, federated_train_data)\n",
        "  #print('round {:2d}, metrics={}'.format(round_num, result.metrics['train']['loss']))\n",
        "  result = training_process.next(state, federated_train_data)\n",
        "  state = result.state\n",
        "  train_metrics = result.metrics['client_work']['train']\n",
        "  print('round {:2d}, metrics={}'.format(round_num, result.metrics['client_work']['train']))\n",
        "'''\n",
        "\n",
        "\n",
        "####\n",
        "print(training_process.initialize.type_signature.formatted_representation())\n",
        "state = training_process.initialize()\n",
        "for round_num in range(1):\n",
        "  #state, metrics = training_process.next(state, federated_train_data)\n",
        "  #print('round {:2d}, metrics={}'.format(round_num, result.metrics['train']['loss']))\n",
        "  result = training_process.next(state, federated_train_data)\n",
        "  state = result.state\n",
        "  print('round {:2d}, metrics={}'.format(round_num, result.metrics))\n",
        "\n",
        "####\n",
        "print(tff.federated_computation(lambda: 'Hello, World!')())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxDlWxfZvXsM",
        "outputId": "2af997a1-2deb-409a-a375-10d8abbe8f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1107396 (4.22 MB)\n",
            "Trainable params: 1107396 (4.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1107396 (4.22 MB)\n",
            "Trainable params: 1107396 (4.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1107396 (4.22 MB)\n",
            "Trainable params: 1107396 (4.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "( -> <\n",
            "  global_model_weights=<\n",
            "    trainable=<\n",
            "      float32[5,5,3,32],\n",
            "      float32[32],\n",
            "      float32[5,5,32,64],\n",
            "      float32[64],\n",
            "      float32[1024,1024],\n",
            "      float32[1024],\n",
            "      float32[1024,4],\n",
            "      float32[4]\n",
            "    >,\n",
            "    non_trainable=<>\n",
            "  >,\n",
            "  distributor=<>,\n",
            "  client_work=<>,\n",
            "  aggregator=<\n",
            "    value_sum_process=<>,\n",
            "    weight_sum_process=<>\n",
            "  >,\n",
            "  finalizer=<\n",
            "    int64,\n",
            "    float32[5,5,3,32],\n",
            "    float32[5,5,3,32],\n",
            "    float32[32],\n",
            "    float32[32],\n",
            "    float32[5,5,32,64],\n",
            "    float32[5,5,32,64],\n",
            "    float32[64],\n",
            "    float32[64],\n",
            "    float32[1024,1024],\n",
            "    float32[1024,1024],\n",
            "    float32[1024],\n",
            "    float32[1024],\n",
            "    float32[1024,4],\n",
            "    float32[1024,4],\n",
            "    float32[4],\n",
            "    float32[4]\n",
            "  >\n",
            ">@SERVER)\n",
            "round  0, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.8999599), ('loss', 1.6106926), ('num_examples', 49890), ('num_batches', 2520)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "b'Hello, World!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state=training_process.initialize()\n",
        "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "print(type(state))\n",
        "print(evaluation)\n",
        "federated_test_data = [preprocessed_client4_dataset]\n",
        "test_metrics = evaluation(training_process.get_model_weights(state), federated_test_data)\n",
        "#test_metrics = evaluation(state.model, federated_train_data, metrics=['sparse_categorical_accuracy', 'loss'])\n",
        "print('test_metrics', test_metrics)"
      ],
      "metadata": {
        "id": "w6jqtfJOJ1Yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552adfe9-6e42-4b0b-fae9-2942d8024416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-da82cb84e8a4>:2: DeprecationWarning: `tff.learning.build_federated_evaluation` is deprecated, use `tff.learning.algorithms.build_fed_eval` instead.\n",
            "  evaluation = tff.learning.build_federated_evaluation(model_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1107396 (4.22 MB)\n",
            "Trainable params: 1107396 (4.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 64)          51264     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1107396 (4.22 MB)\n",
            "Trainable params: 1107396 (4.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "<class 'tensorflow_federated.python.learning.templates.composers.LearningAlgorithmState'>\n",
            "<tensorflow_federated.python.core.impl.computation.computation_impl.ConcreteComputation object at 0x782befff0c10>\n",
            "test_metrics OrderedDict([('eval', OrderedDict([('sparse_categorical_accuracy', 0.10036101), ('loss', 11.798208), ('num_examples', 5540), ('num_batches', 280)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# input test"
      ],
      "metadata": {
        "id": "jNPix-Y2HRgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\"gdrive/My Drive/AI_S_HW4/Train\",batch_size=1, image_size=(28,28), labels='inferred')\n",
        "test_dataset=tf.keras.preprocessing.image_dataset_from_directory(\"gdrive/My Drive/AI_S_HW4/Test\",batch_size=1, image_size=(28,28), labels='inferred')\n",
        "print(type(train_dataset))\n",
        "print(type(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLUbpScOInFq",
        "outputId": "edb6c346-6a26-4087-af92-8a7c4b8222d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4989 files belonging to 4 classes.\n",
            "Found 555 files belonging to 4 classes.\n",
            "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n",
            "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client_size = int(len(train_dataset)*.16)\n",
        "client1_dataset = train_dataset.take(client1_size)\n",
        "client2_dataset = train_dataset.skip(client1_size*1).take(client_size)\n",
        "client3_dataset = train_dataset.skip(client1_size*2).take(client_size)\n",
        "client4_dataset = train_dataset.skip(client1_size*3).take(client_size)\n",
        "client5_dataset = train_dataset.skip(client1_size*4).take(client_size)\n",
        "client6_dataset = train_dataset.skip(client1_size*5).take(client_size)\n",
        "\n",
        "\n",
        "con_client1_dataset=client1_dataset.concatenate(client4_dataset)\n",
        "con_client2_dataset=client2_dataset.concatenate(client5_dataset)\n",
        "con_client3_dataset=client3_dataset.concatenate(client6_dataset)"
      ],
      "metadata": {
        "id": "GMl59dvaoNXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def convert_dataset_to_numpy(dataset):\n",
        "    data_array = []\n",
        "    label_array = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        data_array.append(images.numpy())\n",
        "        label_array.append(labels.numpy())\n",
        "\n",
        "    # 将列表转换为 NumPy 数组\n",
        "    numpy_data_array = np.array(data_array)\n",
        "    numpy_label_array = np.array(label_array)\n",
        "\n",
        "    return numpy_data_array, numpy_label_array\n",
        "\n",
        "# 假设 test_dataset 是你的数据集\n",
        "test_data_array, test_label_array = convert_dataset_to_numpy(test_dataset)\n",
        "train_data_array_1, train_label_array_1 = convert_dataset_to_numpy(con_client1_dataset)\n",
        "train_data_array_2, train_label_array_2 = convert_dataset_to_numpy(con_client2_dataset)\n",
        "train_data_array_3, train_label_array_3 = convert_dataset_to_numpy(con_client3_dataset)\n",
        "# 打印数组的形状\n",
        "print(\"Shape of data array:\", test_data_array.shape)\n",
        "print(\"Shape of label array:\", test_label_array.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL0Iyy5rq5yl",
        "outputId": "32e21582-f676-42d6-96d1-89477d4a47ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data array: (555, 1, 28, 28, 3)\n",
            "Shape of label array: (555, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "class MyClientData(tff.simulation.datasets.ClientData):\n",
        "    def __init__(self, my_dataset):  # 传入你自己的数据集\n",
        "        self.my_dataset = my_dataset\n",
        "\n",
        "    @property\n",
        "    def client_ids(self):\n",
        "        # 返回你的数据集中的客户端 ID 列表\n",
        "        return ['client_1', 'client_2', 'client_3','test_data']\n",
        "\n",
        "    def create_tf_dataset_for_client(self, client_id):\n",
        "        # 根据客户端 ID 返回相应的 TensorFlow 数据集\n",
        "        # 这里的示例假设你的数据集是一个字典，包含 'data' 和 'label' 字段\n",
        "        return tf.data.Dataset.from_tensor_slices({\n",
        "            'data': self.my_dataset[client_id]['data'],\n",
        "            'label': self.my_dataset[client_id]['label']\n",
        "        })\n",
        "    def element_type_structure(self):\n",
        "        # 返回元素的结构，这里的示例假设数据集元素是一个字典\n",
        "        return {'data': tf.TensorSpec(shape=(1, 28, 28), dtype=tf.float32),\n",
        "                'label': tf.TensorSpec(shape=(1,), dtype=tf.int32)}\n",
        "\n",
        "    def serializable_dataset_fn(self):\n",
        "        # 返回一个序列化数据集的函数，这里简单返回数据集本身\n",
        "        return lambda: self.my_dataset\n",
        "\n",
        "# 创建你的数据集\n",
        "my_dataset = {\n",
        "    'client_1': {'data':train_data_array_1, 'label':train_label_array_1},  # 你的实际数据\n",
        "    'client_2': {'data':train_data_array_2, 'label':train_label_array_2},\n",
        "    'client_3': {'data':train_data_array_3, 'label':train_label_array_3},\n",
        "    'test_data': {'data':test_data_array, 'label':test_label_array},\n",
        "}"
      ],
      "metadata": {
        "id": "bNDuwL-4cJ8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "# 创建你的客户端数据集对象\n",
        "my_client_data = MyClientData(my_dataset)\n",
        "\n",
        "# 然后你可以使用上面的代码来访问客户端数据集\n",
        "client_id = my_client_data.client_ids[0]\n",
        "client_local_dataset = tfds.as_numpy(my_client_data.create_tf_dataset_for_client(client_id))"
      ],
      "metadata": {
        "id": "p9qO8Flmv6x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 沒有使用到的code"
      ],
      "metadata": {
        "id": "hnIOpJzYw1Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "for client_id in range(0,3):\n",
        "  client_local_dataset = tfds.as_numpy(emnist_train.create_tf_dataset_for_client(client_id))\n",
        "  # client_local_dataset is an iterable of structures of numpy arrays\n",
        "  for example in client_local_dataset:\n",
        "    print(example)"
      ],
      "metadata": {
        "id": "iHfsVYNqbNlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 取得第一個批次的數據\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(\"Data type:\", type(images))  # images 的類型\n",
        "    print(\"Label type:\", type(labels))  # labels 的類型\n",
        "    print(\"Shape of images:\", images.shape)  # images 的形狀\n",
        "    print(\"Shape of labels:\", labels.shape)  # labels 的形狀"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGckBpWNUeYh",
        "outputId": "480f871d-6753-4668-aa21-dc2db65ef4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Label type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Shape of images: (1, 28, 28, 3)\n",
            "Shape of labels: (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element1,element2):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return (tf.reshape(element1, [28, 28, 3]),\n",
        "            tf.reshape(element2, [1]))\n",
        "\n",
        "  return dataset.map(batch_format_fn).batch(20).repeat(5)"
      ],
      "metadata": {
        "id": "IyJT0ZV2QAL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dfNJQ0DRg4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client1_size = int(len(train_dataset)*.3)\n",
        "client2_size = int(len(train_dataset)*.3)\n",
        "client3_size = int(len(train_dataset)*.3)\n",
        "client4_size = int(len(train_dataset)*.1)\n",
        "client1_dataset = dataset.take(client1_size)\n",
        "client2_dataset = dataset.skip(client1_size).take(client2_size)\n",
        "client3_dataset = dataset.skip(client1_size+client2_size).take(client3_size)\n",
        "client4_dataset = dataset.skip(client1_size+client2_size+client3_size).take(client4_size)\n",
        "print(type(client1_dataset))\n",
        "print(type(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlxtnaGZMQeO",
        "outputId": "92aa2afb-dfe1-41d3-a9d0-b6559f74119d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.take_op._TakeDataset'>\n",
            "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_client1_dataset = preprocess(client1_dataset)\n",
        "preprocessed_client2_dataset = preprocess(client2_dataset)\n",
        "preprocessed_client3_dataset = preprocess(client3_dataset)\n",
        "preprocessed_client4_dataset = preprocess(client4_dataset)\n",
        "print(type(preprocessed_client4_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx7pK6X2RkUZ",
        "outputId": "977e0046-d6f7-435d-b9f7-518da29e9c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.repeat_op._RepeatDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import os\n",
        "\n",
        "# 假設你的圖片數據存儲在這個主文件夾中\n",
        "data_root = '/path/to/your/data'\n",
        "\n",
        "# 將數據組織成 tf.data.Dataset\n",
        "def create_tf_dataset_for_client(client_id):\n",
        "    client_folder = os.path.join(data_root, client_id)\n",
        "    file_paths = [os.path.join(client_folder, file) for file in os.listdir(client_folder)]\n",
        "\n",
        "    # 讀取和解碼圖片，根據你的實際需求進行修改\n",
        "    def _parse_function(file_path):\n",
        "        image = tf.io.read_file(file_path)\n",
        "        image = tf.image.decode_image(image)\n",
        "        # 其他的圖片處理邏輯...\n",
        "\n",
        "        return image\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "    dataset = dataset.map(_parse_function)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# 將數據封裝成 ClientData 對象\n",
        "class MyClientData(tff.simulation.datasets.ClientData):\n",
        "    def __init__(self, data_root):\n",
        "        self.data_root = data_root\n",
        "        self.client_ids = [folder for folder in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, folder))]\n",
        "\n",
        "    def create_tf_dataset_for_client(self, client_id):\n",
        "        return create_tf_dataset_for_client(client_id)\n",
        "\n",
        "# 創建 ClientData 對象\n",
        "my_client_data = MyClientData(data_root)\n",
        "\n",
        "# 現在，my_client_data 就是一個 tff.simulation.datasets.ClientData 對象，可以在聯邦學習中使用"
      ],
      "metadata": {
        "id": "LHNX3LHQJNPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# 定義虛構的客戶端數據\n",
        "client_data_dict = {\n",
        "    'client_1': tf.data.Dataset.from_tensor_slices(([1.0, 2.0], [3.0, 4.0])),\n",
        "    'client_2': tf.data.Dataset.from_tensor_slices(([5.0, 6.0], [7.0, 8.0])),\n",
        "}\n",
        "\n",
        "# 將數據封裝為ClientData對象\n",
        "class MyClientData(tff.simulation.datasets.ClientData):\n",
        "    def __init__(self, client_data_dict):\n",
        "        self.client_data_dict = client_data_dict\n",
        "\n",
        "    @property\n",
        "    def client_ids(self):\n",
        "        return list(self.client_data_dict.keys())\n",
        "\n",
        "    def create_tf_dataset_for_client(self, client_id):\n",
        "        return self.client_data_dict[client_id]\n",
        "\n",
        "# 創建ClientData對象\n",
        "my_client_data = MyClientData(client_data_dict)\n",
        "\n",
        "# 現在，my_client_data 就是一個 tff.simulation.datasets.ClientData 對象，可以在聯邦學習中使用\n"
      ],
      "metadata": {
        "id": "aayegmK5HPk3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}